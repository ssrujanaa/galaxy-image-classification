{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pip\n",
      "  Downloading pip-21.0.1-py3-none-any.whl (1.5 MB)\n",
      "\u001b[K     |████████████████████████████████| 1.5 MB 1.5 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting tensorflow\n",
      "  Downloading tensorflow-2.4.1-cp36-cp36m-manylinux2010_x86_64.whl (394.3 MB)\n",
      "\u001b[K     |████████████████████████████████| 394.3 MB 39 kB/s s eta 0:00:01  |▊                               | 8.9 MB 634 kB/s eta 0:10:08     |█                               | 12.8 MB 678 kB/s eta 0:09:23     |██                              | 26.0 MB 3.7 MB/s eta 0:01:41     |██▋                             | 32.6 MB 8.2 MB/s eta 0:00:45     |███▍                            | 41.4 MB 10.3 MB/s eta 0:00:35     |███▍                            | 42.2 MB 10.3 MB/s eta 0:00:35     |███▌                            | 42.7 MB 10.3 MB/s eta 0:00:35     |████▎                           | 52.3 MB 6.0 MB/s eta 0:00:57     |█████▎                          | 65.3 MB 15.6 MB/s eta 0:00:22     |█████▊                          | 70.6 MB 24.4 MB/s eta 0:00:14     |███████                         | 87.1 MB 22.0 MB/s eta 0:00:14\n",
      "\u001b[?25hCollecting keras\n",
      "  Downloading Keras-2.4.3-py2.py3-none-any.whl (36 kB)\n",
      "Collecting numpy\n",
      "  Downloading numpy-1.19.5-cp36-cp36m-manylinux2010_x86_64.whl (14.8 MB)\n",
      "\u001b[K     |████████████████████████████████| 14.8 MB 32.6 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting pandas\n",
      "  Downloading pandas-1.1.5-cp36-cp36m-manylinux1_x86_64.whl (9.5 MB)\n",
      "\u001b[K     |████████████████████████████████| 9.5 MB 9.0 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting pillow\n",
      "  Downloading Pillow-8.1.2-cp36-cp36m-manylinux1_x86_64.whl (2.2 MB)\n",
      "\u001b[K     |████████████████████████████████| 2.2 MB 1.1 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting sklearn\n",
      "  Downloading sklearn-0.0.tar.gz (1.1 kB)\n",
      "Collecting opencv-python\n",
      "  Downloading opencv_python-4.5.1.48-cp36-cp36m-manylinux2014_x86_64.whl (50.4 MB)\n",
      "\u001b[K     |████████████████████████████████| 50.4 MB 16.6 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting optuna\n",
      "  Downloading optuna-2.6.0-py3-none-any.whl (293 kB)\n",
      "\u001b[K     |████████████████████████████████| 293 kB 3.9 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting scikit-image\n",
      "  Downloading scikit_image-0.17.2-cp36-cp36m-manylinux1_x86_64.whl (12.4 MB)\n",
      "\u001b[K     |████████████████████████████████| 12.4 MB 3.9 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting optkeras\n",
      "  Downloading optkeras-0.0.7-py3-none-any.whl (6.9 kB)\n",
      "Collecting h5py==2.10.0\n",
      "  Downloading h5py-2.10.0-cp36-cp36m-manylinux1_x86_64.whl (2.9 MB)\n",
      "\u001b[K     |████████████████████████████████| 2.9 MB 27.6 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting flatbuffers~=1.12.0\n",
      "  Downloading flatbuffers-1.12-py2.py3-none-any.whl (15 kB)\n",
      "Collecting tensorflow-estimator<2.5.0,>=2.4.0\n",
      "  Downloading tensorflow_estimator-2.4.0-py2.py3-none-any.whl (462 kB)\n",
      "\u001b[K     |████████████████████████████████| 462 kB 20.4 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting google-pasta~=0.2\n",
      "  Downloading google_pasta-0.2.0-py3-none-any.whl (57 kB)\n",
      "\u001b[K     |████████████████████████████████| 57 kB 8.0 MB/s  eta 0:00:01\n",
      "\u001b[?25hCollecting keras-preprocessing~=1.1.2\n",
      "  Downloading Keras_Preprocessing-1.1.2-py2.py3-none-any.whl (42 kB)\n",
      "\u001b[K     |████████████████████████████████| 42 kB 2.2 MB/s  eta 0:00:01\n",
      "\u001b[?25hCollecting astunparse~=1.6.3\n",
      "  Downloading astunparse-1.6.3-py2.py3-none-any.whl (12 kB)\n",
      "Collecting protobuf>=3.9.2\n",
      "  Downloading protobuf-3.15.5-cp36-cp36m-manylinux1_x86_64.whl (1.0 MB)\n",
      "\u001b[K     |████████████████████████████████| 1.0 MB 17.2 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting six~=1.15.0\n",
      "  Downloading six-1.15.0-py2.py3-none-any.whl (10 kB)\n",
      "Collecting typing-extensions~=3.7.4\n",
      "  Downloading typing_extensions-3.7.4.3-py3-none-any.whl (22 kB)\n",
      "Collecting grpcio~=1.32.0\n",
      "  Downloading grpcio-1.32.0-cp36-cp36m-manylinux2014_x86_64.whl (3.8 MB)\n",
      "\u001b[K     |████████████████████████████████| 3.8 MB 32.3 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting absl-py~=0.10\n",
      "  Downloading absl_py-0.11.0-py3-none-any.whl (127 kB)\n",
      "\u001b[K     |████████████████████████████████| 127 kB 7.8 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting wheel~=0.35\n",
      "  Downloading wheel-0.36.2-py2.py3-none-any.whl (35 kB)\n",
      "Collecting wrapt~=1.12.1\n",
      "  Downloading wrapt-1.12.1.tar.gz (27 kB)\n",
      "Collecting termcolor~=1.1.0\n",
      "  Downloading termcolor-1.1.0.tar.gz (3.9 kB)\n",
      "Collecting gast==0.3.3\n",
      "  Downloading gast-0.3.3-py2.py3-none-any.whl (9.7 kB)\n",
      "Collecting opt-einsum~=3.3.0\n",
      "  Downloading opt_einsum-3.3.0-py3-none-any.whl (65 kB)\n",
      "\u001b[K     |████████████████████████████████| 65 kB 6.1 MB/s  eta 0:00:01\n",
      "\u001b[?25hCollecting tensorboard~=2.4\n",
      "  Downloading tensorboard-2.4.1-py3-none-any.whl (10.6 MB)\n",
      "\u001b[K     |████████████████████████████████| 10.6 MB 5.3 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied, skipping upgrade: pyyaml in /usr/lib64/python3.6/site-packages (from keras) (3.13)\n",
      "Collecting scipy>=0.14\n",
      "  Downloading scipy-1.5.4-cp36-cp36m-manylinux1_x86_64.whl (25.9 MB)\n",
      "\u001b[K     |████████████████████████████████| 25.9 MB 371 kB/s  eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied, skipping upgrade: python-dateutil>=2.7.3 in /usr/local/lib/python3.6/site-packages (from pandas) (2.8.1)\n",
      "Requirement already satisfied, skipping upgrade: pytz>=2017.2 in /usr/local/lib/python3.6/site-packages (from pandas) (2020.1)\n",
      "Collecting scikit-learn\n",
      "  Downloading scikit_learn-0.24.1-cp36-cp36m-manylinux2010_x86_64.whl (22.2 MB)\n",
      "\u001b[K     |████████████████████████████████| 22.2 MB 3.7 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting colorlog\n",
      "  Downloading colorlog-4.7.2-py2.py3-none-any.whl (10 kB)\n",
      "Collecting cliff\n",
      "  Downloading cliff-3.7.0-py3-none-any.whl (80 kB)\n",
      "\u001b[K     |████████████████████████████████| 80 kB 3.7 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting alembic\n",
      "  Downloading alembic-1.5.6-py2.py3-none-any.whl (159 kB)\n",
      "\u001b[K     |████████████████████████████████| 159 kB 26.6 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting sqlalchemy>=1.1.0\n",
      "  Downloading SQLAlchemy-1.3.23-cp36-cp36m-manylinux2010_x86_64.whl (1.3 MB)\n",
      "\u001b[K     |████████████████████████████████| 1.3 MB 28.7 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied, skipping upgrade: packaging>=20.0 in /usr/local/lib/python3.6/site-packages (from optuna) (20.4)\n",
      "Collecting cmaes>=0.8.2\n",
      "  Downloading cmaes-0.8.2-py3-none-any.whl (15 kB)\n",
      "Collecting tqdm\n",
      "  Downloading tqdm-4.59.0-py2.py3-none-any.whl (74 kB)\n",
      "\u001b[K     |████████████████████████████████| 74 kB 2.7 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting matplotlib!=3.0.0,>=2.0.0\n",
      "  Downloading matplotlib-3.3.4-cp36-cp36m-manylinux1_x86_64.whl (11.5 MB)\n",
      "\u001b[K     |████████████████████████████████| 11.5 MB 15.0 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting tifffile>=2019.7.26\n",
      "  Downloading tifffile-2020.9.3-py3-none-any.whl (148 kB)\n",
      "\u001b[K     |████████████████████████████████| 148 kB 3.4 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting imageio>=2.3.0\n",
      "  Downloading imageio-2.9.0-py3-none-any.whl (3.3 MB)\n",
      "\u001b[K     |████████████████████████████████| 3.3 MB 22.6 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting networkx>=2.0\n",
      "  Downloading networkx-2.5-py3-none-any.whl (1.6 MB)\n",
      "\u001b[K     |████████████████████████████████| 1.6 MB 47.1 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting PyWavelets>=1.1.1\n",
      "  Downloading PyWavelets-1.1.1-cp36-cp36m-manylinux1_x86_64.whl (4.4 MB)\n",
      "\u001b[K     |████████████████████████████████| 4.4 MB 3.9 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting google-auth<2,>=1.6.3\n",
      "  Downloading google_auth-1.27.1-py2.py3-none-any.whl (136 kB)\n",
      "\u001b[K     |████████████████████████████████| 136 kB 11.4 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting markdown>=2.6.8\n",
      "  Downloading Markdown-3.3.4-py3-none-any.whl (97 kB)\n",
      "\u001b[K     |████████████████████████████████| 97 kB 7.0 MB/s  eta 0:00:01\n",
      "\u001b[?25hCollecting setuptools>=41.0.0\n",
      "  Downloading setuptools-54.1.1-py3-none-any.whl (784 kB)\n",
      "\u001b[K     |████████████████████████████████| 784 kB 21.0 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied, skipping upgrade: requests<3,>=2.21.0 in /usr/local/lib/python3.6/site-packages (from tensorboard~=2.4->tensorflow) (2.24.0)\n",
      "Collecting tensorboard-plugin-wit>=1.6.0\n",
      "  Downloading tensorboard_plugin_wit-1.8.0-py3-none-any.whl (781 kB)\n",
      "\u001b[K     |████████████████████████████████| 781 kB 3.5 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting google-auth-oauthlib<0.5,>=0.4.1\n",
      "  Downloading google_auth_oauthlib-0.4.3-py2.py3-none-any.whl (18 kB)\n",
      "Collecting werkzeug>=0.11.15\n",
      "  Downloading Werkzeug-1.0.1-py2.py3-none-any.whl (298 kB)\n",
      "\u001b[K     |████████████████████████████████| 298 kB 4.4 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting joblib>=0.11\n",
      "  Downloading joblib-1.0.1-py3-none-any.whl (303 kB)\n",
      "\u001b[K     |████████████████████████████████| 303 kB 4.1 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting threadpoolctl>=2.0.0\n",
      "  Downloading threadpoolctl-2.1.0-py3-none-any.whl (12 kB)\n",
      "Collecting cmd2>=1.0.0\n",
      "  Downloading cmd2-1.5.0-py3-none-any.whl (133 kB)\n",
      "\u001b[K     |████████████████████████████████| 133 kB 28.8 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting PrettyTable>=0.7.2\n",
      "  Downloading prettytable-2.1.0-py3-none-any.whl (22 kB)\n",
      "Collecting pbr!=2.1.0,>=2.0.0\n",
      "  Downloading pbr-5.5.1-py2.py3-none-any.whl (106 kB)\n",
      "\u001b[K     |████████████████████████████████| 106 kB 324 kB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied, skipping upgrade: pyparsing>=2.1.0 in /usr/local/lib/python3.6/site-packages (from cliff->optuna) (2.4.7)\n",
      "Collecting stevedore>=2.0.1\n",
      "  Downloading stevedore-3.3.0-py3-none-any.whl (49 kB)\n",
      "\u001b[K     |████████████████████████████████| 49 kB 2.9 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting Mako\n",
      "  Downloading Mako-1.1.4-py2.py3-none-any.whl (75 kB)\n",
      "\u001b[K     |████████████████████████████████| 75 kB 6.4 MB/s  eta 0:00:01\n",
      "\u001b[?25hCollecting python-editor>=0.3\n",
      "  Downloading python_editor-1.0.4-py3-none-any.whl (4.9 kB)\n",
      "Collecting kiwisolver>=1.0.1\n",
      "  Downloading kiwisolver-1.3.1-cp36-cp36m-manylinux1_x86_64.whl (1.1 MB)\n",
      "\u001b[K     |████████████████████████████████| 1.1 MB 2.0 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting cycler>=0.10\n",
      "  Downloading cycler-0.10.0-py2.py3-none-any.whl (6.5 kB)\n",
      "Requirement already satisfied, skipping upgrade: decorator>=4.3.0 in /usr/local/lib/python3.6/site-packages (from networkx>=2.0->scikit-image) (4.4.2)\n",
      "Collecting pyasn1-modules>=0.2.1\n",
      "  Downloading pyasn1_modules-0.2.8-py2.py3-none-any.whl (155 kB)\n",
      "\u001b[K     |████████████████████████████████| 155 kB 5.3 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting rsa<5,>=3.1.4; python_version >= \"3.6\"\n",
      "  Downloading rsa-4.7.2-py3-none-any.whl (34 kB)\n",
      "Collecting cachetools<5.0,>=2.0.0\n",
      "  Downloading cachetools-4.2.1-py3-none-any.whl (12 kB)\n",
      "Requirement already satisfied, skipping upgrade: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.6/site-packages (from markdown>=2.6.8->tensorboard~=2.4->tensorflow) (1.7.0)\n",
      "Requirement already satisfied, skipping upgrade: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/site-packages (from requests<3,>=2.21.0->tensorboard~=2.4->tensorflow) (3.0.4)\n",
      "Requirement already satisfied, skipping upgrade: idna<3,>=2.5 in /usr/lib/python3.6/site-packages (from requests<3,>=2.21.0->tensorboard~=2.4->tensorflow) (2.7)\n",
      "Requirement already satisfied, skipping upgrade: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/site-packages (from requests<3,>=2.21.0->tensorboard~=2.4->tensorflow) (1.25.10)\n",
      "Requirement already satisfied, skipping upgrade: certifi>=2017.4.17 in /usr/local/lib/python3.6/site-packages (from requests<3,>=2.21.0->tensorboard~=2.4->tensorflow) (2020.6.20)\n",
      "Collecting requests-oauthlib>=0.7.0\n",
      "  Downloading requests_oauthlib-1.3.0-py2.py3-none-any.whl (23 kB)\n",
      "Requirement already satisfied, skipping upgrade: wcwidth>=0.1.7 in /usr/local/lib/python3.6/site-packages (from cmd2>=1.0.0->cliff->optuna) (0.2.5)\n",
      "Collecting pyperclip>=1.6\n",
      "  Downloading pyperclip-1.8.2.tar.gz (20 kB)\n",
      "Requirement already satisfied, skipping upgrade: attrs>=16.3.0 in /usr/local/lib/python3.6/site-packages (from cmd2>=1.0.0->cliff->optuna) (20.1.0)\n",
      "Collecting colorama>=0.3.7\n",
      "  Downloading colorama-0.4.4-py2.py3-none-any.whl (16 kB)\n",
      "Requirement already satisfied, skipping upgrade: MarkupSafe>=0.9.2 in /usr/local/lib64/python3.6/site-packages (from Mako->alembic->optuna) (1.1.1)\n",
      "Collecting pyasn1<0.5.0,>=0.4.6\n",
      "  Downloading pyasn1-0.4.8-py2.py3-none-any.whl (77 kB)\n",
      "\u001b[K     |████████████████████████████████| 77 kB 8.6 MB/s  eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied, skipping upgrade: zipp>=0.5 in /usr/local/lib/python3.6/site-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard~=2.4->tensorflow) (3.1.0)\n",
      "Collecting oauthlib>=3.0.0\n",
      "  Downloading oauthlib-3.1.0-py2.py3-none-any.whl (147 kB)\n",
      "\u001b[K     |████████████████████████████████| 147 kB 11.4 MB/s eta 0:00:01\n",
      "\u001b[?25hUsing legacy 'setup.py install' for sklearn, since package 'wheel' is not installed.\n",
      "Using legacy 'setup.py install' for wrapt, since package 'wheel' is not installed.\n",
      "Using legacy 'setup.py install' for termcolor, since package 'wheel' is not installed.\n",
      "Using legacy 'setup.py install' for pyperclip, since package 'wheel' is not installed.\n",
      "Installing collected packages: pip, flatbuffers, tensorflow-estimator, six, google-pasta, numpy, keras-preprocessing, wheel, astunparse, h5py, protobuf, typing-extensions, grpcio, absl-py, wrapt, termcolor, gast, opt-einsum, pyasn1, pyasn1-modules, rsa, setuptools, cachetools, google-auth, markdown, tensorboard-plugin-wit, oauthlib, requests-oauthlib, google-auth-oauthlib, werkzeug, tensorboard, tensorflow, scipy, keras, pandas, pillow, joblib, threadpoolctl, scikit-learn, sklearn, opencv-python, colorlog, pyperclip, colorama, cmd2, PrettyTable, pbr, stevedore, cliff, Mako, sqlalchemy, python-editor, alembic, cmaes, tqdm, optuna, kiwisolver, cycler, matplotlib, tifffile, imageio, networkx, PyWavelets, scikit-image, optkeras\n",
      "  Attempting uninstall: pip\n",
      "    Found existing installation: pip 20.2.2\n",
      "    Uninstalling pip-20.2.2:\n",
      "      Successfully uninstalled pip-20.2.2\n",
      "  Attempting uninstall: six\n",
      "    Found existing installation: six 1.14.0\n",
      "    Uninstalling six-1.14.0:\n",
      "      Successfully uninstalled six-1.14.0\n",
      "    Running setup.py install for wrapt ... \u001b[?25ldone\n",
      "\u001b[?25h    Running setup.py install for termcolor ... \u001b[?25ldone\n",
      "\u001b[?25h  Attempting uninstall: setuptools\n",
      "    Found existing installation: setuptools 39.2.0\n",
      "    Uninstalling setuptools-39.2.0:\n",
      "      Successfully uninstalled setuptools-39.2.0\n",
      "    Running setup.py install for sklearn ... \u001b[?25ldone\n",
      "\u001b[?25h    Running setup.py install for pyperclip ... \u001b[?25ldone\n",
      "\u001b[?25hSuccessfully installed Mako-1.1.4 PrettyTable-2.1.0 PyWavelets-1.1.1 absl-py-0.11.0 alembic-1.5.6 astunparse-1.6.3 cachetools-4.2.1 cliff-3.7.0 cmaes-0.8.2 cmd2-1.5.0 colorama-0.4.4 colorlog-4.7.2 cycler-0.10.0 flatbuffers-1.12 gast-0.3.3 google-auth-1.27.1 google-auth-oauthlib-0.4.3 google-pasta-0.2.0 grpcio-1.32.0 h5py-2.10.0 imageio-2.9.0 joblib-1.0.1 keras-2.4.3 keras-preprocessing-1.1.2 kiwisolver-1.3.1 markdown-3.3.4 matplotlib-3.3.4 networkx-2.5 numpy-1.19.5 oauthlib-3.1.0 opencv-python-4.5.1.48 opt-einsum-3.3.0 optkeras-0.0.7 optuna-2.6.0 pandas-1.1.5 pbr-5.5.1 pillow-8.1.2 pip-21.0.1 protobuf-3.15.5 pyasn1-0.4.8 pyasn1-modules-0.2.8 pyperclip-1.8.2 python-editor-1.0.4 requests-oauthlib-1.3.0 rsa-4.7.2 scikit-image-0.17.2 scikit-learn-0.24.1 scipy-1.5.4 setuptools-54.1.1 six-1.15.0 sklearn-0.0 sqlalchemy-1.3.23 stevedore-3.3.0 tensorboard-2.4.1 tensorboard-plugin-wit-1.8.0 tensorflow-2.4.1 tensorflow-estimator-2.4.0 termcolor-1.1.0 threadpoolctl-2.1.0 tifffile-2020.9.3 tqdm-4.59.0 typing-extensions-3.7.4.3 werkzeug-1.0.1 wheel-0.36.2 wrapt-1.12.1\n"
     ]
    }
   ],
   "source": [
    "!sudo pip3 install --upgrade pip tensorflow keras numpy pandas pillow sklearn opencv-python optuna scikit-image  optkeras h5py==2.10.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !sudo pip3 install keras==2.1.5 tensorflow==1.13.1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Pegasus.api.workflow.Workflow at 0x7fe917d72d30>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "import os\n",
    "import logging\n",
    "from pathlib import Path\n",
    "import requests \n",
    "from glob import glob\n",
    "from zipfile import ZipFile\n",
    "import pickle\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "logging.basicConfig(level=logging.DEBUG)\n",
    "\n",
    "#Import Pegasus API\n",
    "from Pegasus.api import *\n",
    "props = Properties()\n",
    "props[\"pegasus.mode\"] = \"development\"\n",
    "props.write()\n",
    "\n",
    "#Replica Catalog\n",
    "rc = ReplicaCatalog()\n",
    "in_files=[]\n",
    "\n",
    "label_csv = './inputs/training_solutions_rev1.csv'\n",
    "\n",
    "for file in glob(\"./inputs/*.jpg\"):\n",
    "    f = file.replace(\"./inputs/\", '')\n",
    "    in_files.append(File(f))\n",
    "    rc.add_replica(\"local\", File(f), Path(\"./inputs/\").resolve() / f)\n",
    "    \n",
    "csv = label_csv.replace(\"./inputs/\",'')\n",
    "rc.add_replica(\"local\", File(csv), Path(\"./inputs/\").resolve() / csv)\n",
    "rc.write()\n",
    "#Transformation\n",
    "\n",
    "data_label = Transformation( \"Step1.py\",\n",
    "            site=\"local\",\n",
    "            pfn=\"/home/scitech/shared-data/Galaxy2/Step1.py\",\n",
    "            is_stageable=True\n",
    "            )\n",
    "training_preprocess1 = Transformation( \"Training_Preprocess1.py\",\n",
    "            site=\"local\",\n",
    "            pfn=\"/home/scitech/shared-data/Galaxy2/Training_Preprocess1.py\",\n",
    "            is_stageable=True\n",
    "            )\n",
    "training_preprocess2 = Transformation( \"Training_Preprocess2.py\",\n",
    "            site=\"local\",\n",
    "            pfn=\"/home/scitech/shared-data/Galaxy2/Training_Preprocess2.py\",\n",
    "            is_stageable=True\n",
    "            )\n",
    "val_preprocess = Transformation( \"Val_Preprocess.py\",\n",
    "            site=\"local\",\n",
    "            pfn=\"/home/scitech/shared-data/Galaxy2/Val_Preprocess.py\",\n",
    "            is_stageable=True\n",
    "            )\n",
    "test_preprocess = Transformation( \"Test_Preprocess.py\",\n",
    "            site=\"local\",\n",
    "            pfn=\"/home/scitech/shared-data/Galaxy2/Test_Preprocess.py\",\n",
    "            is_stageable=True\n",
    "            )\n",
    "tc = TransformationCatalog()\\\n",
    "    .add_transformations(data_label,training_preprocess1,training_preprocess2,val_preprocess, test_preprocess)\\\n",
    "    .write()\n",
    "\n",
    "#Workflow\n",
    "wf = Workflow(\"Galaxy\", infer_dependencies=True)\n",
    "\n",
    "dataset_without_file = []\n",
    "y = []\n",
    "for i in range(5):\n",
    "    dataset_without_file.append('Class0_'+ str(i)+'.jpg')\n",
    "    y.append(0)\n",
    "    dataset_without_file.append('Class1_'+ str(i)+'.jpg')\n",
    "    y.append(1)\n",
    "    dataset_without_file.append('Class2_'+ str(i)+'.jpg')\n",
    "    y.append(2)\n",
    "    dataset_without_file.append('Class3_'+ str(i)+'.jpg')\n",
    "    y.append(3)\n",
    "    dataset_without_file.append('Class4_'+ str(i)+'.jpg')\n",
    "    y.append(4)\n",
    "\n",
    "dataset = []\n",
    "for i in range(len(dataset_without_file)):\n",
    "    dataset.append(File(dataset_without_file[i]))\n",
    "\n",
    "job_data_label = Job(data_label)\\\n",
    "                    .add_inputs(*in_files, File(csv))\\\n",
    "                    .add_outputs(*dataset)\n",
    "        \n",
    "X = dataset_without_file\n",
    "\n",
    "X_train1, X_test, y_train1, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify = y)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train1, y_train1, test_size=0.3, random_state=42, stratify = y_train1)\n",
    "\n",
    "train_images = []\n",
    "for i in range(len(X_train)):\n",
    "    train_images.append(File(X_train[i]))\n",
    "train_image_file1 = File(\"Training_images_Preprocess1.npy\")\n",
    "train_label_file1 = File(\"Training_labels_Preprocess1.npy\")\n",
    "job_training_preprocess1 = Job(training_preprocess1)\\\n",
    "                    .add_inputs(*train_images)\\\n",
    "                    .add_outputs(train_image_file1,train_label_file1)\n",
    "\n",
    "train_image_file2 = File(\"Training_images_Preprocess2.npy\")\n",
    "train_label_file2 = File(\"Training_labels_Preprocess2.npy\")\n",
    "job_training_preprocess2 = Job(training_preprocess2)\\\n",
    "                    .add_inputs(train_image_file1,train_label_file1)\\\n",
    "                    .add_outputs(train_image_file2,train_label_file2)\n",
    "val_images = []\n",
    "for i in range(len(X_val)):\n",
    "    val_images.append(File(X_val[i]))\n",
    "val_image_file = File(\"Val_images_Preprocess.npy\")\n",
    "val_label_file = File(\"Val_labels_Preprocess.npy\")\n",
    "job_val_preprocess = Job(val_preprocess)\\\n",
    "                    .add_inputs(*val_images)\\\n",
    "                    .add_outputs(val_image_file,val_label_file)\n",
    "\n",
    "test_images = []\n",
    "for i in range(len(X_test)):\n",
    "    test_images.append(File(X_test[i]))\n",
    "test_image_file = File(\"Test_images_Preprocess.npy\")\n",
    "test_label_file = File(\"Test_labels_Preprocess.npy\")\n",
    "job_test_preprocess = Job(test_preprocess)\\\n",
    "                    .add_inputs(*test_images)\\\n",
    "                    .add_outputs(test_image_file,test_label_file)\n",
    "\n",
    "wf.add_jobs(job_data_label,job_training_preprocess1,job_training_preprocess2,job_val_preprocess,job_test_preprocess)                                    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(X_test),len(X_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "################\n",
      "# pegasus-plan #\n",
      "################\n",
      "[main] WARN  schema.JsonMetaSchema  - Unknown keyword $defs - you should define your own Meta Schema. If the keyword is irrelevant for validation, just use a NonValidationKeyword\n",
      "[main] WARN  schema.JsonMetaSchema  - Unknown keyword additionalItems - you should define your own Meta Schema. If the keyword is irrelevant for validation, just use a NonValidationKeyword\n",
      "[main] WARN  schema.JsonMetaSchema  - Unknown keyword examples - you should define your own Meta Schema. If the keyword is irrelevant for validation, just use a NonValidationKeyword\n",
      "2021.03.09 21:07:17.901 UTC:\n",
      "2021.03.09 21:07:17.907 UTC:   -----------------------------------------------------------------------\n",
      "2021.03.09 21:07:17.913 UTC:   File for submitting this DAG to HTCondor           : Galaxy-0.dag.condor.sub\n",
      "2021.03.09 21:07:17.919 UTC:   Log of DAGMan debugging messages                 : Galaxy-0.dag.dagman.out\n",
      "2021.03.09 21:07:17.924 UTC:   Log of HTCondor library output                     : Galaxy-0.dag.lib.out\n",
      "2021.03.09 21:07:17.930 UTC:   Log of HTCondor library error messages             : Galaxy-0.dag.lib.err\n",
      "2021.03.09 21:07:17.937 UTC:   Log of the life of condor_dagman itself          : Galaxy-0.dag.dagman.log\n",
      "2021.03.09 21:07:17.942 UTC:\n",
      "2021.03.09 21:07:17.949 UTC:   -no_submit given, not submitting DAG to HTCondor.  You can do this with:\n",
      "2021.03.09 21:07:17.960 UTC:   -----------------------------------------------------------------------\n",
      "2021.03.09 21:07:18.878 UTC:   Your database is compatible with Pegasus version: 5.0.0dev\n",
      "2021.03.09 21:07:20.398 UTC:   Created Pegasus database in: sqlite:////home/scitech/shared-data/Galaxy2/scitech/pegasus/Galaxy/run0002/Galaxy-0.replicas.db\n",
      "2021.03.09 21:07:20.404 UTC:   Your database is compatible with Pegasus version: 5.0.0dev\n",
      "2021.03.09 21:07:20.462 UTC:   Output replica catalog set to jdbc:sqlite:/home/scitech/shared-data/Galaxy2/scitech/pegasus/Galaxy/run0002/Galaxy-0.replicas.db\n",
      "2021.03.09 21:07:20.679 UTC:   Submitting to condor Galaxy-0.dag.condor.sub\n",
      "2021.03.09 21:07:20.706 UTC:\n",
      "2021.03.09 21:07:20.711 UTC:   Your workflow has been started and is running in the base directory:\n",
      "2021.03.09 21:07:20.716 UTC:\n",
      "2021.03.09 21:07:20.722 UTC:   /home/scitech/shared-data/Galaxy2/scitech/pegasus/Galaxy/run0002\n",
      "2021.03.09 21:07:20.728 UTC:\n",
      "2021.03.09 21:07:20.733 UTC:   *** To monitor the workflow you can run ***\n",
      "2021.03.09 21:07:20.739 UTC:\n",
      "2021.03.09 21:07:20.744 UTC:   pegasus-status -l /home/scitech/shared-data/Galaxy2/scitech/pegasus/Galaxy/run0002\n",
      "2021.03.09 21:07:20.749 UTC:\n",
      "2021.03.09 21:07:20.755 UTC:   *** To remove your workflow run ***\n",
      "2021.03.09 21:07:20.761 UTC:\n",
      "2021.03.09 21:07:20.766 UTC:   pegasus-remove /home/scitech/shared-data/Galaxy2/scitech/pegasus/Galaxy/run0002\n",
      "2021.03.09 21:07:21.030 UTC:   Time taken to execute is 4.906 seconds\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\u001b[1;32m##################################################\u001b[0m] 100.0% ..Success (\u001b[1;32mCompleted: 21\u001b[0m, \u001b[1;33mQueued: 0\u001b[0m, \u001b[1;36mRunning: 0\u001b[0m, \u001b[1;31mFailed: 0\u001b[0m)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "####################\n",
      "# pegasus-analyzer #\n",
      "####################\n",
      "Your database is compatible with Pegasus version: 5.0.0dev\n",
      "\n",
      "************************************Summary*************************************\n",
      "\n",
      "Submit Directory   : /home/scitech/shared-data/Galaxy2/scitech/pegasus/Galaxy/run0002\n",
      "Total jobs         :     21 (100.00%)\n",
      "# jobs succeeded   :     21 (100.00%)\n",
      "# jobs failed      :      0 (0.00%)\n",
      "# jobs held        :      0 (0.00%)\n",
      "# jobs unsubmitted :      0 (0.00%)\n",
      "\n",
      "\n",
      "\n",
      "######################\n",
      "# pegasus-statistics #\n",
      "######################\n",
      "Your database is compatible with Pegasus version: 5.0.0dev\n",
      "\n",
      "#\n",
      "# Pegasus Workflow Management System - http://pegasus.isi.edu\n",
      "#\n",
      "# Workflow summary:\n",
      "#   Summary of the workflow execution. It shows total\n",
      "#   tasks/jobs/sub workflows run, how many succeeded/failed etc.\n",
      "#   In case of hierarchical workflow the calculation shows the\n",
      "#   statistics across all the sub workflows.It shows the following\n",
      "#   statistics about tasks, jobs and sub workflows.\n",
      "#     * Succeeded - total count of succeeded tasks/jobs/sub workflows.\n",
      "#     * Failed - total count of failed tasks/jobs/sub workflows.\n",
      "#     * Incomplete - total count of tasks/jobs/sub workflows that are\n",
      "#       not in succeeded or failed state. This includes all the jobs\n",
      "#       that are not submitted, submitted but not completed etc. This\n",
      "#       is calculated as  difference between 'total' count and sum of\n",
      "#       'succeeded' and 'failed' count.\n",
      "#     * Total - total count of tasks/jobs/sub workflows.\n",
      "#     * Retries - total retry count of tasks/jobs/sub workflows.\n",
      "#     * Total+Retries - total count of tasks/jobs/sub workflows executed\n",
      "#       during workflow run. This is the cumulative of retries,\n",
      "#       succeeded and failed count.\n",
      "# Workflow wall time:\n",
      "#   The wall time from the start of the workflow execution to the end as\n",
      "#   reported by the DAGMAN.In case of rescue dag the value is the\n",
      "#   cumulative of all retries.\n",
      "# Cumulative job wall time:\n",
      "#   The sum of the wall time of all jobs as reported by kickstart.\n",
      "#   In case of job retries the value is the cumulative of all retries.\n",
      "#   For workflows having sub workflow jobs (i.e SUBDAG and SUBDAX jobs),\n",
      "#   the wall time value includes jobs from the sub workflows as well.\n",
      "# Cumulative job wall time as seen from submit side:\n",
      "#   The sum of the wall time of all jobs as reported by DAGMan.\n",
      "#   This is similar to the regular cumulative job wall time, but includes\n",
      "#   job management overhead and delays. In case of job retries the value\n",
      "#   is the cumulative of all retries. For workflows having sub workflow\n",
      "#   jobs (i.e SUBDAG and SUBDAX jobs), the wall time value includes jobs\n",
      "#   from the sub workflows as well.\n",
      "# Cumulative job badput wall time:\n",
      "#   The sum of the wall time of all failed jobs as reported by kickstart.\n",
      "#   In case of job retries the value is the cumulative of all retries.\n",
      "#   For workflows having sub workflow jobs (i.e SUBDAG and SUBDAX jobs),\n",
      "#   the wall time value includes jobs from the sub workflows as well.\n",
      "# Cumulative job badput wall time as seen from submit side:\n",
      "#   The sum of the wall time of all failed jobs as reported by DAGMan.\n",
      "#   This is similar to the regular cumulative job badput wall time, but includes\n",
      "#   job management overhead and delays. In case of job retries the value\n",
      "#   is the cumulative of all retries. For workflows having sub workflow\n",
      "#   jobs (i.e SUBDAG and SUBDAX jobs), the wall time value includes jobs\n",
      "#   from the sub workflows as well.\n",
      "------------------------------------------------------------------------------\n",
      "Type           Succeeded Failed  Incomplete  Total     Retries   Total+Retries\n",
      "Tasks          5         0       0           5         0         5\n",
      "Jobs           21        0       0           21        0         21\n",
      "Sub-Workflows  0         0       0           0         0         0\n",
      "------------------------------------------------------------------------------\n",
      "\n",
      "Workflow wall time                                       : 4 mins, 10 secs\n",
      "Cumulative job wall time                                 : 3 mins, 5 secs\n",
      "Cumulative job wall time as seen from submit side        : 3 mins, 9 secs\n",
      "Cumulative job badput wall time                          : 0.0 secs\n",
      "Cumulative job badput wall time as seen from submit side : 0.0 secs\n",
      "\n",
      "# Integrity Metrics\n",
      "# Number of files for which checksums were compared/computed along with total time spent doing it.\n",
      "91 files checksums compared with total duration of 3.72 secs\n",
      "64 files checksums generated with total duration of 0.47 secs\n",
      "\n",
      "# Integrity Errors\n",
      "# Total:\n",
      "#       Total number of integrity errors encountered across all job executions(including retries) of a workflow.\n",
      "# Failures:\n",
      "#       Number of failed jobs where the last job instance had integrity errors.\n",
      "Failures: 0 job failures had integrity errors\n",
      "\n",
      "Summary                       : /home/scitech/shared-data/Galaxy2/scitech/pegasus/Galaxy/run0002/statistics/summary.txt\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "     wf.plan(submit=True)\\\n",
    "        .wait()\\\n",
    "        .analyze()\\\n",
    "        .statistics()\n",
    "except PegasusClientError as e:\n",
    "    print(e.output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
